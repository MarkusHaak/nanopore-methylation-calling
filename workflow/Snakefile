import os, sys, glob
import re
import pandas as pd
from snakemake.utils import min_version
import yaml
import shutil

from scripts.get_workflow import get_guppy_config_name, config_name_to_pore

##### run this workflow #####
# snakemake --cores 24 --resources gpu=1 tombo=1 --use-conda --sdm conda --use-singularity --keep-going --default-resources "tmpdir='/vol/data/tmp/'" --config sample=<some/sample>
# --nt --keep-incomplete --config tombo_per_read_stats=true
# -n --dag | grep -v ERROR | tail -n +4 | dot -Tpdf > dag.pdf
#############################

##### set minimum snakemake version #####
min_version("8.0.0")

conda:
    "envs/global.yaml"

configfile: "config/config.yaml"

DATASETS = (
    pd.read_csv(config["datasets"])
    .set_index("ID", drop=False)
    .sort_index()
)

SSH_SERVERS = (
    pd.read_csv(config["ssh_servers"])
    .set_index("alias", drop=False)
    .sort_index()
)

def find_any_file(wildcards, filetype=".fast5", d=None):
    if d is None:
        d = f"{config['datasets_dir']}/{config['sample']}/NAT_raw/"
    all_files = glob.glob(f"{d}/**/*{filetype}", recursive=True)
    if len(all_files) > 0:
        return all_files[0]
    else:
        return "missing"

def extract_meta_field(fn, field):
    with open(fn, 'r') as f:
        for line in f.readlines():
            if field in line:
                try:
                    return re.search(f"{field}: ([^,]*)[ ]*\n", line).group(1)
                except:
                    return None
    return None

def parse_pore(wildcards):
    fn = f"results/{config['sample']}/pore.txt"
    with open(fn, 'r') as f:
        return f.read().strip()

def all_model_paths(models=None):
    model_paths = {}
    if models is None:
        models = []
        for pore in config['models']:
            if 'r10' in pore:
                for rate in config['models'][pore]:
                    for model in [config['models'][pore][rate]['simplex']] + config['models'][pore][rate]['modifications']:
                        models.append(model)
            elif 'r9' in pore:
                for model in [config['models'][pore]['modifications']]:
                    models.append(model)
    elif type(models) == str:
        models = [models]
    
    for model in models:
        if "r10" in model:
            if model.startswith('res'):
                model_paths[model] = f"resources/rerio/dorado_models/{model}"
            elif model.startswith('dna'):
                model_paths[model] = f"resources/dorado_models/{model}"
            else:
                raise ValueError("unknown model name format")
        elif "r9" in model:
            if model.startswith('res'): # r9 simplex models are purposely not returned since no path needs to be given
                model_paths[model] = f"resources/ont-guppy/data/{model}.cfg"
        else:
            raise ValueError("unknown model name format")
    return list(model_paths.values())

def get_dorado_models_from_config(wildcards, model_type):
    pore = parse_pore(wildcards)
    if pore not in config['models'] or pore.startswith('r9'):
        print(f"ERROR: No dorado models set in config for pore {pore}")
        return ""
    meta_fn = f"results/{config['sample']}/metadata.yaml"
    sample_rate = extract_meta_field(meta_fn, 'sample_rate')
    if sample_rate is None:
        print(f"ERROR: no sample_rate field in {meta_fn}")
        return ""
    elif int(sample_rate) not in config['models'][pore]:
        print(f"ERROR: No dorado models set in config for pore {pore} with sample_rate {sample_rate}")
        return ""
    if model_type not in config['models'][pore][int(sample_rate)]:
        print(f"ERROR: No '{model_type}' dorado models set in config for pore {pore} with sample_rate {sample_rate}")
        return ""
    models = all_model_paths(models=config['models'][pore][int(sample_rate)][model_type])
    if type(models) == list:
        models = ",".join(models)
    return models

def get_guppy_models_from_config(wildcards, model_type):
    pore = parse_pore(wildcards)
    if pore not in config['models'] or pore.startswith('r10'):
        print(f"ERROR: No guppy models set in config for pore {pore}")
        return ""
    if model_type not in config['models'][pore]:
        print(f"ERROR: No '{model_type}' guppy models set in config for pore {pore}")
        return ""
    models = all_model_paths(models=config['models'][pore][model_type])
    return models

def parse_raw_dtype(wildcards):
    with open(f"results/{config['sample']}/raw_dtype.txt", "r") as f:
        return f.read().strip()

def parse_config(wildcards):
    with open(f"results/{config['sample']}/config.txt", "r") as f:
        return f.read().strip() + '.cfg'

def get_raw_dir(wildcards):
    filetype = parse_raw_dtype(wildcards)
    raw_files = glob.glob(f"{config['datasets_dir']}/{config['sample']}/NAT_raw/**/*.{filetype}", recursive=True)
    return os.path.dirname(raw_files[0])

##### target rules #####

rule all:
    input:
        f"results/{config['sample']}/workflow_command.txt",
        f"results/{config['sample']}/workflow_version.txt",
        f"results/{config['sample']}/workflow_config.yaml",
        f"results/{config['sample']}/dataset_entry.yaml",
        f"results/{config['sample']}/success.r10.txt",
        f"results/{config['sample']}/success.r9.txt",
        f"results/{config['sample']}/success.r10_comparative.txt",
        f"results/{config['sample']}/success.r9_comparative.txt"

rule r10_analysis:
    input:
        f"results/{config['sample']}/starttime.txt",
        f"results/{config['sample']}/prokka",
        f"results/{config['sample']}/NAT_dorado.contig_depth.csv",
        f"results/{config['sample']}/nanoplot/NAT_dorado/",
        f"results/{config['sample']}/MicrobeMod/NAT_dorado.rm.genes.tsv",
        f"results/{config['sample']}/MicrobeMod/NAT_dorado.motifs.tsv",
        f"results/{config['sample']}/modkit/NAT_dorado.modkit_pileup.auto_filter.bed",
        f"results/{config['sample']}/modkit/NAT_dorado.modkit_pileup.fixed_filter.bed",
        f"results/{config['sample']}/modkit/NAT_dorado.modkit_pileup.no_filter.bed",
    output:
        success = f"results/{config['sample']}/success.r10.txt"
    shell:
        ("""
        touch {output.success}
        start=$(cat {input[0]})
        end=`date +%s`
        runtime=$((end-start))
        echo $runtime > results/{config[sample]}/total_runtime.txt
        """)

rule r9_analysis:
    input:
        f"results/{config['sample']}/starttime.txt",
        f"results/{config['sample']}/prokka",
        f"results/{config['sample']}/NAT_guppy_canonical.contig_depth.csv",
        f"results/{config['sample']}/nanoplot/NAT_guppy_canonical/",
        f"results/{config['sample']}/MicrobeMod/NAT_guppy_canonical.rm.genes.tsv",
        f"results/{config['sample']}/tombo/alt.runtime.txt",
        f"results/{config['sample']}/tombo/de_novo.runtime.txt",
        f"results/{config['sample']}/modbam2bed/NAT_guppy_modified.modbam2bed.bed",
        f"results/{config['sample']}/modkit/NAT_guppy_modified.modkit_pileup.auto_filter.bed",
        f"results/{config['sample']}/modkit/NAT_guppy_modified.modkit_pileup.fixed_filter.bed",
        f"results/{config['sample']}/modkit/NAT_guppy_modified.modkit_pileup.no_filter.bed",
    output:
        success = f"results/{config['sample']}/success.r9.txt"
    shell:
        ("""
        touch {output.success}
        start=$(cat {input[0]})
        end=`date +%s`
        runtime=$((end-start))
        echo $runtime > results/{config[sample]}/total_runtime.txt
        """)

rule r10_comparative_analysis:
    input:
        f"results/{config['sample']}/starttime.txt",
        expand(
        f"results/{config['sample']}/{{raw_set}}_dorado.contig_depth.csv", raw_set=["NAT", "WGA"]
        ),
        expand(
        f"results/{config['sample']}/nanoplot/{{raw_set}}_dorado/", raw_set=["NAT", "WGA"]
        ),
    output:
        success = f"results/{config['sample']}/success.r10_comparative.txt"
    shell:
        ("""
        touch {output.success}
        start=$(cat {input[0]})
        end=`date +%s`
        runtime=$((end-start))
        echo $runtime > results/{config[sample]}/total_runtime.txt
        """)

rule r9_comparative_analysis:
    input:
        f"results/{config['sample']}/starttime.txt",
        expand(
        f"results/{config['sample']}/{{raw_set}}_guppy_canonical.contig_depth.csv", raw_set=["NAT", "WGA"]
        ),
        expand(
        f"results/{config['sample']}/nanoplot/{{raw_set}}_guppy_canonical/", raw_set=["NAT", "WGA"]
        ),
        f"results/{config['sample']}/snapper/",
        f"results/{config['sample']}/nanodisco/difference/",
        f"results/{config['sample']}/tombo/compare.runtime.txt",
    output:
        success = f"results/{config['sample']}/success.r9_comparative.txt"
    shell:
        ("""
        touch {output.success}
        start=$(cat {input[0]})
        end=`date +%s`
        runtime=$((end-start))
        echo $runtime > results/{config[sample]}/total_runtime.txt
        """)

##### load rules #####

include: "rules/common.smk"
include: "rules/basecalling.smk"
include: "rules/fast5.smk"
include: "rules/fetch_data.smk"
include: "rules/mapping.smk"
include: "rules/MicrobeMod.smk"
include: "rules/modbam.smk"
include: "rules/nanodisco.smk"
include: "rules/setup.smk"
include: "rules/snapper.smk"
include: "rules/stats_and_annotation.smk"
include: "rules/tombo.smk"

##### rules with run directive #####

rule download_and_extract_raw_data:
    input:
        f"results/{config['sample']}/starttime.txt"
    output:
        temp(directory(f"resources/datasets/{config['sample']}/{{raw_set}}_raw/"))
    params:
        source = lambda wildcards: DATASETS.loc[config['sample'], f'{wildcards.raw_set}_source']
    run:
        import os
        if params.source.startswith('http') and "tar.gz" in params.source:
            shell(
                f"""
                mkdir -p {output}
                cd {output}
                wget -r -np -nH --cut-dirs 20 {params.source}
                tar -xzf *.tar.gz*
                rm *.tar.gz*
                """)
        elif params.source.startswith('s3'):
            shell(
                f"""
                aws s3 cp --recursive --no-sign-request {params.source} {output} \
                --exclude "*" --include "{os.path.basename(config['sample'])}*"
                """)
        elif ':' in params.source:
            # assume ssh
            for alias in SSH_SERVERS.index:
                if params.source.startswith(alias+":"):
                    fp_server = params.source.split(":")[1]
                    shell(
                        f"""
                        rsync -av \
                        -e "ssh -i {SSH_SERVERS.loc[alias, 'identity']} -p {SSH_SERVERS.loc[alias, 'port']}" \
                        --include="*/" --include="*.fast5" \
                        --include="*.pod5" --exclude="*" \
                        {SSH_SERVERS.loc[alias, 'user']}@{SSH_SERVERS.loc[alias, 'address']}:{fp_server} {output}/
                        """)
        elif '/' in params.source and os.path.exists(params.source):
            # assume local files
            shell(
                f"""
                mkdir -p {output}
                cp -r {params.source} {output}/
                """)

rule download_reference:
    input:
        f"resources/datasets/{config['sample']}/NAT_raw",
        f"results/{config['sample']}/starttime.txt"
    output:
        temp(directory(f"resources/datasets/{config['sample']}/reference/"))
    params:
        source = DATASETS.loc[config['sample'], 'ref_source']
    run:
        import os
        if params.source.startswith('http'):
            if "tar.gz" in params.source:
                shell(
                    f"""
                    mkdir -p {output}
                    cd resources/datasets/{config['sample']}
                    wget -r -np -nH --cut-dirs 20 {params.source}
                    tar -xzf *.tar.gz* -C reference/
                    rm *.tar.gz*
                    """)
            else:
                shell(
                    f"""
                    mkdir -p {output}
                    cd {output}
                    wget -r -np -nH --cut-dirs 20 {params.source}
                    """)
        elif params.source.startswith('s3'):
            shell(
                f"""
                aws s3 cp --recursive --no-sign-request {params.source} {output} \
                --exclude "*" --include "{os.path.basename(config['sample'])}*"
                """)
        elif ':' in params.source:
            # assume ssh
            for alias in SSH_SERVERS.index:
                if params.source.startswith(alias+":"):
                    fp_server = params.source.split(":")[1]
                    shell(
                        f"""
                        mkdir -p {output}
                        scp -r -P {SSH_SERVERS.loc[alias, 'port']} -i {SSH_SERVERS.loc[alias, 'identity']} {SSH_SERVERS.loc[alias, 'user']}@{SSH_SERVERS.loc[alias, 'address']}:{fp_server} {output}/
                        """)
        elif '/' in params.source and os.path.exists(params.source):
            # assume local files
            shell(
                f"""
                mkdir -p {output}
                cp -r {params.source} {output}/
                """)
        else:
            # assume NCBI nuccore accession
            shell(
                f"""
                mkdir -p {output}
                esearch -db nuccore -query "{params.source}" | efetch -format fasta > "{output}/{params.source}.fasta"
                """)

rule export_dataset_entry:
    output:
        f"results/{config['sample']}/dataset_entry.yaml"
    params:
        ds_entry = DATASETS.loc[config['sample']].to_dict()
    run:
        import os, yaml
        os.makedirs(os.path.dirname(f"{output}"), exist_ok=True)
        with open(f"{output}", 'w+') as ff:
            yaml.dump(params.ds_entry, ff)
        

